{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets_and_benchmarks Group\n",
    "### Collect existing datasets and benchmarks:\n",
    "- Images Classification: MNIST,CIFAR,ImageNet\n",
    "- Image Segmentation: VOC2012-2014\n",
    "- Videos data: Kinetics 400, UCF101\n",
    "- Neural data: NLB, Sensorium, MTNeuro, TUH, HCP\n",
    "\n",
    "### Steps to pre-process data:\n",
    "- Data cleaning: Clean and remove any inconsistent or missing data\n",
    "- Data normalization: Normalize the data so that all features have the same scale and units.\n",
    "- Feature extraction: Extract meaningful features from the raw data\n",
    "- Data augmentation\n",
    "- Data splitting: Split the data into training, validation, and test sets\n",
    "\n",
    "### Publish new datasets and benchmarks: (NeurIPS guidelines)\n",
    "- Formatting: data format and read/write tools\n",
    "- Documentation: instructions to use the data and associated tools\n",
    "- Maintenance: how the datasets are archived and updated over time\n",
    "- Ethics: biases in the data, personal identifiable information within the data\n",
    "- Licensing: how the data can be distributed for research/commercial purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import all packages here\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load_data:\n",
    "    def __init__(self, key, categray=None):\n",
    "            self.key = key\n",
    "            self.categray = categray\n",
    "    def collect_data(self):\n",
    "            if self.key == 'MNIST':\n",
    "                transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                transforms.Normalize((0.1307,), (0.3081,))])\n",
    "                train_dataset = datasets.MNIST(root='../data/MNIST', train=False, download=True,\n",
    "                                       transform=transform)\n",
    "                test_dataset = datasets.MNIST(root='../data/MNIST', train=False,\n",
    "                                      transform=transform)\n",
    "            if self.key == 'cifar':\n",
    "                transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                transforms.Normalize((0.5, 0.5, 0.5), \n",
    "                                                                     (0.5, 0.5, 0.5))])\n",
    "                trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "                testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "            return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pre_process:\n",
    "    def __init__(self, train_dataset, test_dataset, pre_process_list):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.pre_process_list = pre_process_list\n",
    "    def pre_process_function(self):\n",
    "        \"reseize, crop, normalize, or augment images\"\n",
    "        trainloader = torch.utils.data.DataLoader(train_dataset,batch_size=bs, shuffle=True)\n",
    "        testloader = torch.utils.data.DataLoader(test_dataset,batch_size=bs, shuffle=True)\n",
    "        return trainloader, testloader\n",
    "    \n",
    "    def show_single_class(self, dl,label):\n",
    "        dataiter = iter(dl)\n",
    "        imgs, lbls = dataiter.next()\n",
    "        for i in range(100):  # show just the frogs\n",
    "            if lbls[i] == label:  # 6 = frog\n",
    "                self.imshow(torchvision.utils.make_grid(imgs[i]))\n",
    "    \n",
    "    def imshow(self):\n",
    "        img = img / 2 + 0.5   # unnormalize\n",
    "        npimg = img.numpy()   # convert from tensor\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0))) \n",
    "        plt.show()\n",
    "    \n",
    "    def show_batch(sefl, dl):\n",
    "        # DL: DATA Loader\n",
    "        for images, labels in dl:\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n",
    "            break\n",
    "\n",
    "    def missing_data_proc(self, df, type='mean', del_cols = []):\n",
    "        \"\"\"Process missing data.\n",
    "        Arguments - df - input dataset as a dataframe,\n",
    "        type - The type of missing data imputation to be done,\n",
    "        del_cols - columns to be deleted, if type == 'Delete Columns'.\"\"\"\n",
    "\n",
    "        if type == 'mean':\n",
    "            df = df.fillna(df.mean())\n",
    "        elif type == 'median':\n",
    "            df = df.fillna(df.median())\n",
    "        elif type == 'mode':\n",
    "            df = df.fillna(df.mode())\n",
    "        elif type == 'Delete Columns':\n",
    "            df.drop(del_cols, axis=1)\n",
    "        elif type == 'MICE':\n",
    "            mice_imputer = IterativeImputer()\n",
    "            imputed_data = mice_imputer.fit_transform(df)\n",
    "            df = pd.DataFrame(imputed_data, columns=df.columns)\n",
    "        elif type == 'KNN':\n",
    "            knn_imputer = KNNImputer(n_neighbors=5)\n",
    "            imputed_data = knn_imputer.fit_transform(df)\n",
    "            df = pd.DataFrame(imputed_data, columns=df.columns)\n",
    "        else:\n",
    "            print(\"Please enter a valid type of Missing data processing.\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse_577",
   "language": "python",
   "name": "cse_577"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
